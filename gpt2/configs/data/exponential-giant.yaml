tokenizer: "NoLossTokenizer"
dataset_name: "giant-mid-coarse"
gradient_accumulation_steps: 20  # used to simulate larger batch sizes
batch_size: 32  # if gradient_accumulation_steps > 1, this is the micro-batch size
max_iters: 6000  # total number of training iterations, ~20 times giant dataset
augmentation_probability: 0